{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_model(model_path=\"Qwen/Qwen2-0.5B-Instruct\"):\n",
    "    device = \"cpu\" # the device to load the model onto\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        pad_token_id=0\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(modelPath=\"sentence-transformers/all-MiniLM-L12-v2\"):\n",
    "    # Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "    model_kwargs = {'device':'cpu'}\n",
    "\n",
    "    # Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "    # Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=modelPath,     # Provide the pre-trained model's path\n",
    "        model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "        encode_kwargs=encode_kwargs # Pass the encoding options\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Retrieve Relevant Information\n",
    "def retrieve_context(query, top_k=3):\n",
    "    # Load FAISS index from local storage\n",
    "    vector_store = FAISS.load_local(\"faiss_index\", embeddings=embeddings(), allow_dangerous_deserialization=True)\n",
    "    \"\"\"Retrieve the most relevant documents for a given query.\"\"\"\n",
    "    docs = vector_store.similarity_search(query, k=top_k)\n",
    "    context = \" \".join([doc.page_content for doc in docs])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Your main role is to answer questions from the user. You are an assistant specializing in computer science principles and coding.\n",
    "Retrieve relevant information from the dataset and utilize inference and suggestions for the following tasks:\n",
    "- Responses should cover fundamental principles of computer science.\n",
    "- Inferences are allowed to provide comprehensive answers.\n",
    "- Use the provided context to list down relevant information and explanations.\n",
    "- Ensure all responses are accurate and aligned with computer science topics.\n",
    "Ensure responses are derived from the dataset, use inference and suggestions to provide comprehensive answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(user_query):\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(user_query)\n",
    "    model, tokenizer = RAG_model()\n",
    "    \n",
    "    # Prepare the prompt with context\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_template},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\n{user_query}\"}\n",
    "    ]\n",
    "    \n",
    "    # Concatenate the messages into a single string for the model\n",
    "    text = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in messages])\n",
    "    \n",
    "    # Tokenize and generate response\n",
    "    model_inputs = tokenizer(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        pad_token_id=tokenizer.eos_token_id  # To avoid potential padding issues\n",
    "    )\n",
    "\n",
    "    # Decode the generated response\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Extract the response after the user query\n",
    "    response_start = generated_text.find(\"Answer:\")\n",
    "    if response_start != -1:\n",
    "        cleaned_response = generated_text[response_start + len(\"Answer:\"):].strip()\n",
    "    else:\n",
    "        cleaned_response = generated_text.strip()\n",
    "\n",
    "    cleaned_response = \"\\n\\n\".join([line.strip() for line in cleaned_response.split(\"\\n\\n\") if line.strip()])\n",
    "    \n",
    "    return print(\"Response: \",cleaned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "user_query = \"What is EC2 in AWS?\"\n",
    "ask_question(user_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
